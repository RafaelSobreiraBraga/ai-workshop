{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Machine Learning Demonstration\n",
    "\n",
    "\n",
    "https://aws.amazon.com/pt/machine-learning/\n",
    "Amazon Machine Learning is a service that makes it easy for developers of all skill levels to use machine learning technology. Amazon Machine Learning provides visualization tools and wizards that guide you through the process of creating machine learning (ML) models without having to learn complex ML algorithms and technology.\n",
    "With Amazon Machine Learning you can train three different types of models, using the following algorithms:\n",
    " - Binary Logistic Regression\n",
    " - Multinomial Logistic Regression\n",
    " - Linear Regression\n",
    " \n",
    "We will use Multinomial Logistic Regression to create a model for predicting the category of a product, given its short descriptiion.\n",
    "\n",
    "Python Boto3 reference:\n",
    "http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: to create a model to predict a given product category\n",
    "\n",
    "Model:\n",
    " - Input: product short description\n",
    " - Output: category\n",
    " - *predict_categoria(product_name) -> category*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import IPython.display as disp\n",
    "import json\n",
    "\n",
    "from time import gmtime, strftime\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import Markdown\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the current Sagemaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Created S3 bucket: sagemaker-us-east-1-715445047862\n"
     ]
    }
   ],
   "source": [
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "client = boto3.client('machinelearning', region_name='us-east-1')\n",
    "s3_client = boto3.client('s3')\n",
    "s3 = boto3.client('s3')\n",
    "base_dir='/tmp/aml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bucket_arn = \"arn:aws:s3:::%s/*\" % s3_bucket\n",
    "policy_statement = {\n",
    "    \"Sid\": \"AddPerm\",\n",
    "    \"Effect\": \"Allow\",\n",
    "    \"Principal\": \"*\",\n",
    "    \"Action\": \"s3:GetObject\",\n",
    "    \"Resource\": bucket_arn\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_policy = None\n",
    "try:\n",
    "    current_policy = json.loads(s3_client.get_bucket_policy(Bucket=s3_bucket)['Policy'])\n",
    "    policy_found = False\n",
    "    for st in current_policy['Statement']:\n",
    "        if st[\"Action\"] == \"s3:GetObject\" and st[\"Resource\"] == bucket_arn:\n",
    "            policy_found = True\n",
    "            break\n",
    "\n",
    "    if not policy_found:\n",
    "        current_policy['Statement'].append( bucket_statement )\n",
    "except Exception as e:\n",
    "    print(\"There is no current policy. Adding one...\")\n",
    "    s3_client.put_bucket_policy(\n",
    "        Bucket=s3_bucket,\n",
    "        Policy=json.dumps(\n",
    "            {\n",
    "                \"Version\": \"2012-10-17\",\n",
    "                \"Statement\": [policy_statement]\n",
    "            }\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist moment\n",
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $base_dir\n",
    "!curl -s https://workshopml.spock.cloud/datasets/products/aml_data.tar.gz | tar -xz -C $base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(base_dir + '/sample.csv', sep=',', encoding='utf-8')\n",
    "print( len(data) )\n",
    "data.iloc[[517, 163, 14, 826, 692]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we need to remove accents, transform everything to lower case and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tranlating table for removing accents\n",
    "accents = \"\".maketrans(\"áàãâéêíóôõúüçÁÀÃÂÉÊÍÓÔÕÚÜÇ\", \"aaaaeeiooouucAAAAEEIOOOUUC\")\n",
    "\n",
    "# loading stopwords without accents\n",
    "file = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = list(map(lambda x:x.strip().translate(accents),file.readlines()))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this tokenizer will tokenize the text, remove stop words and compute bigrams (ngram(2))\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,2), analyzer='word', stop_words=stopwords, token_pattern='[a-zA-Z]+')\n",
    "tokenizer = word_vectorizer.build_tokenizer()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return \" \".join( list(filter( lambda x: x not in stopwords, tokenizer(text) )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['product_name_tokens'] = list(map(lambda x: remove_stop_words( x.lower().translate(accents) ), data['product_name']))\n",
    "data['main_category_tokens'] = list(map(lambda x: remove_stop_words( x.lower().translate(accents) ), data['main_category']))\n",
    "data['subcategory_tokens'] = list(map(lambda x: remove_stop_words( x.lower().translate(accents) ), data['sub_category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.iloc[[26, 163, 14, 826, 692]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's remove the unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_final = data[ [ 'product_name_tokens', 'main_category_tokens', 'subcategory_tokens' ]]\n",
    "data_final = data_final.rename(columns={\n",
    "    \"product_name_tokens\": \"product_name\", \n",
    "    \"main_category_tokens\": \"category\",\n",
    "    \"subcategory_tokens\": \"sub_category\", \n",
    "})\n",
    "data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok. We finished our 'sample' dataset preparation.\n",
    "## Now, lets continue with the dataset that was already cleaned.\n",
    "## In real life, you should apply all these transformations to your final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "disp.Image(base_dir + '/workflow_processo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets execute the steps above, using Amazon Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# First, lets upload our dataset to S3\n",
    "s3.upload_file( base_dir + '/dataset.csv', s3_bucket, 'workshop/AML/dataset.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# just take a look on that, before continue\n",
    "pd.read_csv(base_dir + '/dataset.csv', sep=',', encoding='utf-8').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, lets create the DataSources\n",
    "### Before that, we need to split it into 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strategy_train = open( 'split_strategy_training.json', 'r').read()\n",
    "strategy_test = open( 'split_strategy_test.json', 'r').read()\n",
    "print( \"Training: {}\\nTest: {}\".format( strategy_train, strategy_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How AML knows the file format (CSV)? By using the schema bellow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "categorias_schema = open('category_schema.json', 'r').read()\n",
    "print( \"Formato dos dados do dataset: {}\\n\".format( categorias_schema) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the DataSources (train and test) for the Category Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_datasource_name = 'CategoriasTrain' + '_' + strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "test_datasource_name = 'CategoriasTest' + '_' + strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "\n",
    "print(train_datasource_name, test_datasource_name)\n",
    "\n",
    "resp = client.create_data_source_from_s3(\n",
    "    DataSourceId=train_datasource_name,\n",
    "    DataSourceName=train_datasource_name,\n",
    "    DataSpec={\n",
    "        'DataLocationS3': 's3://%s/workshop/AML/dataset.csv' % s3_bucket,\n",
    "        'DataSchema': categorias_schema,\n",
    "        'DataRearrangement': strategy_train\n",
    "    },\n",
    "    ComputeStatistics=True\n",
    ")\n",
    "\n",
    "resp = client.create_data_source_from_s3(\n",
    "    DataSourceId=test_datasource_name,\n",
    "    DataSourceName=test_datasource_name,\n",
    "    DataSpec={\n",
    "        'DataLocationS3': 's3://%s/workshop/AML/dataset.csv' % s3_bucket,\n",
    "        'DataSchema': categorias_schema,\n",
    "        'DataRearrangement': strategy_test\n",
    "    },\n",
    "    ComputeStatistics=True\n",
    ")\n",
    "\n",
    "waiter = client.get_waiter('data_source_available')\n",
    "waiter.wait(FilterVariable='Name', EQ=train_datasource_name)\n",
    "waiter.wait(FilterVariable='Name', EQ=test_datasource_name)\n",
    "print( \"Datasources created successfully!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating/training the Category model\n",
    "\n",
    "This is the Model Recipe. It contains the last transformations applyed to your dataset before start training the model. Please note the function: ngram(product_name, 2). It will create bigrams for the input text. So, the model will consider as input a term frequency table, extracted from the bigrams of the product_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cat_recipe = open('category_recipe.json', 'r').read()\n",
    "print(cat_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html\n",
    "## The training will start as soon as you execute the command bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = 'ProdutoCategorias' + '_' + strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "print(model_name)\n",
    "resp = client.create_ml_model(\n",
    "    MLModelId=model_name,\n",
    "    MLModelName=model_name,\n",
    "    MLModelType='MULTICLASS',\n",
    "    Parameters={\n",
    "        'sgd.maxPasses': '30',\n",
    "        'sgd.shuffleType': 'auto',\n",
    "        'sgd.l2RegularizationAmount': '1e-6'\n",
    "    },\n",
    "    TrainingDataSourceId=train_datasource_name,\n",
    "    Recipe=cat_recipe\n",
    ")\n",
    "waiter = client.get_waiter('ml_model_available')\n",
    "waiter.wait(FilterVariable='Name', EQ=model_name)\n",
    "print( \"Model created successfully!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eval_name = 'ProdutoCategoriasEval' + '_' + strftime(\"%Y%m%d_%H%M%S\", gmtime())\n",
    "# it will take around 4mins.\n",
    "resp = client.create_evaluation(\n",
    "    EvaluationId=eval_name,\n",
    "    EvaluationName=eval_name,\n",
    "    MLModelId=model_name,\n",
    "    EvaluationDataSourceId=test_datasource_name\n",
    ")\n",
    "waiter = client.get_waiter('evaluation_available')\n",
    "waiter.wait(FilterVariable='Name', EQ=eval_name)\n",
    "print( \"Model evaluated successfully!\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It will take a few more minutes, please check the service console if you wish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the model score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = client.get_evaluation( EvaluationId=eval_name )\n",
    "print(\"Score categorias: {}\".format( score['PerformanceMetrics']['Properties']['MulticlassAvgFScore'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting new Categories with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    client.create_realtime_endpoint(\n",
    "        MLModelId=model_name\n",
    "    )\n",
    "    print('Please, wait a few seconds while the endpoint is being created. Get some coffee...')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_category( product_name ):\n",
    "    response = client.predict(\n",
    "        MLModelId=model_name,\n",
    "        Record={\n",
    "            'product_name': product_name\n",
    "        },\n",
    "        PredictEndpoint='https://realtime.machinelearning.us-east-1.amazonaws.com'\n",
    "    )\n",
    "    return response['Prediction']['predictedLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testes = pd.read_csv(base_dir + '/testes.csv', sep=',', encoding='utf-8')\n",
    "testes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = None\n",
    "try:\n",
    "    testes['predicted_category'] = testes['product_name'].apply(predict_category)\n",
    "    result = testes\n",
    "except Exception as e:\n",
    "    print( \"Your realtime endpoint is not ready yet... Please, wait for a few seconds more and try again.\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "client.delete_realtime_endpoint(MLModelId=model_name)\n",
    "print(\"Endpoint deleted\")\n",
    "client.delete_ml_model(MLModelId=model_name)\n",
    "print(\"Model deleted\")\n",
    "client.delete_evaluation(EvaluationId=eval_name)\n",
    "print(\"Evaluation deleted\")\n",
    "client.delete_data_source(DataSourceId=test_datasource_name)\n",
    "print(\"Datasource deleted\")\n",
    "client.delete_data_source(DataSourceId=train_datasource_name)\n",
    "print(\"Endpoint deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
