{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Machine Learning Demonstration\n",
    "\n",
    "\n",
    "https://aws.amazon.com/pt/machine-learning/\n",
    "Amazon Machine Learning is a service that makes it easy for developers of all skill levels to use machine learning technology. Amazon Machine Learning provides visualization tools and wizards that guide you through the process of creating machine learning (ML) models without having to learn complex ML algorithms and technology.\n",
    "With Amazon Machine Learning you can train three different types of models, using the following algorithms:\n",
    " - Binary Logistic Regression\n",
    " - Multinomial Logistic Regression\n",
    " - Linear Regression\n",
    " \n",
    "We will use Multinomial Logistic Regression to create a model for predicting the category of a product, given its short descriptiion.\n",
    "\n",
    "Python Boto3 reference:\n",
    "http://boto3.readthedocs.io/en/latest/reference/services/machinelearning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal: to create a model to predict a given product category\n",
    "\n",
    "Model:\n",
    " - Input: product short description\n",
    " - Output: category\n",
    " - *predict_categoria(product_name) -> category*\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import boto3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "import IPython.display as disp\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from IPython.display import Markdown\n",
    "from notebook import notebookapp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the current Sagemaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before start running this tutorial, please add the following policy to your bucket\n",
    "```javascript\n",
    "{\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"AddPerm\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": \"*\",\n",
    "            \"Action\": \"s3:GetObject\",\n",
    "            \"Resource\": \"arn:aws:s3:::<YOUR_S3_BUCKET_NAME_HERE>/*\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket = sagemaker_session.default_bucket()\n",
    "client = boto3.client('machinelearning')\n",
    "s3 = boto3.client('s3')\n",
    "base_dir='/tmp/aml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scientist moment\n",
    "## Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p $base_dir\n",
    "!curl -s https://workshopml.spock.cloud/datasets/products/aml_data.tar.gz | tar -xz -C $base_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(base_dir + '/sample.csv', sep=',', encoding='utf-8')\n",
    "print( len(data) )\n",
    "data.iloc[[517, 163, 14, 826, 692]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, we need to remove accents, transform everything to lower case and remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tranlating table for removing accents\n",
    "accents = \"\".maketrans(\"áàãâéêíóôõúüçÁÀÃÂÉÊÍÓÔÕÚÜÇ\", \"aaaaeeiooouucAAAAEEIOOOUUC\")\n",
    "\n",
    "# loading stopwords without accents\n",
    "file = open(\"stopwords.txt\", \"r\")\n",
    "stopwords = list(map(lambda x:x.strip().translate(accents),file.readlines()))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this tokenizer will tokenize the text, remove stop words and compute bigrams (ngram(2))\n",
    "word_vectorizer = TfidfVectorizer(ngram_range=(1,2), analyzer='word', stop_words=stopwords, token_pattern='[a-zA-Z]+')\n",
    "tokenizer = word_vectorizer.build_tokenizer()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    return \" \".join( list(filter( lambda x: x not in stopwords, tokenizer(text) )) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['product_name_tokens'] = list(map(lambda x: remove_stop_words( x.lower().translate(accents) ), data['product_name']))\n",
    "data['main_category_tokens'] = list(map(lambda x: remove_stop_words( x.lower().translate(accents) ), data['main_category']))\n",
    "data['subcategory_tokens'] = list(map(lambda x: remove_stop_words( x.lower().translate(accents) ), data['sub_category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[[26, 163, 14, 826, 692]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's remove the unecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final = data[ [ 'product_name_tokens', 'main_category_tokens', 'subcategory_tokens' ]]\n",
    "data_final = data_final.rename(columns={\n",
    "    \"product_name_tokens\": \"product_name\", \n",
    "    \"main_category_tokens\": \"category\",\n",
    "    \"subcategory_tokens\": \"sub_category\", \n",
    "})\n",
    "data_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ok. We finished our 'sample' dataset preparation.\n",
    "## Now, lets continue with the dataset that was already cleaned.\n",
    "## In real life, you should apply all these transformations to your final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disp.Image(base_dir + '/workflow_processo.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, lets execute the steps above, using Amazon Machine Learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, lets upload our dataset to S3\n",
    "s3.upload_file( base_dir + '/dataset.csv', s3_bucket, 'workshop/AML/dataset.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just take a look on that, before continue\n",
    "pd.read_csv(base_dir + '/dataset.csv', sep=',', encoding='utf-8').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, lets create the DataSources\n",
    "### Before that, we need to split it into 70% training and 30% test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy_train = open( 'split_strategy_training.json', 'r').read()\n",
    "strategy_test = open( 'split_strategy_test.json', 'r').read()\n",
    "print( \"Training: {}\\nTest: {}\".format( strategy_train, strategy_test ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How AML knows the file format (CSV)? By using the schema bellow..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorias_schema = open('category_schema.json', 'r').read()\n",
    "print( \"Formato dos dados do dataset: {}\\n\".format( categorias_schema) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the DataSources (train and test) for the Category Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = client.create_data_source_from_s3(\n",
    "    DataSourceId='ProdCategoriasTrain',\n",
    "    DataSourceName='Dataset de produtos e suas categorias (train)',\n",
    "    DataSpec={\n",
    "        'DataLocationS3': 's3://%s/workshop/AML/dataset.csv' % s3_bucket,\n",
    "        'DataSchema': categorias_schema,\n",
    "        'DataRearrangement': strategy_train\n",
    "    },\n",
    "    ComputeStatistics=True\n",
    ")\n",
    "resp = client.create_data_source_from_s3(\n",
    "    DataSourceId='ProdCategoriasTest',\n",
    "    DataSourceName='Dataset de produtos e suas categorias (test)',\n",
    "    DataSpec={\n",
    "        'DataLocationS3': 's3://%s/workshop/AML/dataset.csv' % s3_bucket,\n",
    "        'DataSchema': categorias_schema,\n",
    "        'DataRearrangement': strategy_test\n",
    "    },\n",
    "    ComputeStatistics=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating/training the Category model\n",
    "\n",
    "This is the Model Recipe. It contains the last transformations applyed to your dataset before start training the model. Please note the function: ngram(product_name, 2). It will create bigrams for the input text. So, the model will consider as input a term frequency table, extracted from the bigrams of the product_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_recipe = open('category_recipe.json', 'r').read()\n",
    "print(cat_recipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: http://docs.aws.amazon.com/machine-learning/latest/dg/data-transformations-reference.html\n",
    "## The training will start as soon as you execute the command bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resp = client.create_ml_model(\n",
    "    MLModelId='ProdutoCategorias',\n",
    "    MLModelName='Modelo de produtos e suas Categorias',\n",
    "    MLModelType='MULTICLASS',\n",
    "    Parameters={\n",
    "        'sgd.maxPasses': '30',\n",
    "        'sgd.shuffleType': 'auto',\n",
    "        'sgd.l2RegularizationAmount': '1e-6'\n",
    "    },\n",
    "    TrainingDataSourceId='ProdCategoriasTrain',\n",
    "    Recipe=cat_recipe\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You must wait for the end of the training, before trying to evaluate it.\n",
    "### You can use your time checking the rest of the code or doing something more interesting.\n",
    "### Come back after 8 or 10 mins and continue executing this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# it will take around 4mins.\n",
    "client.create_evaluation(\n",
    "    EvaluationId='ProdutoCategorias',\n",
    "    EvaluationName='Teste do modelo ProdutoCategorias',\n",
    "    MLModelId='ProdutoCategorias',\n",
    "    EvaluationDataSourceId='ProdCategoriasTest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It will take a few more minutes, please check the service console if you wish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the model score..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "score = client.get_evaluation( EvaluationId='ProdutoCategorias' )\n",
    "print(\"Score categorias: {}\".format( score['PerformanceMetrics']['Properties']['MulticlassAvgFScore'] ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting new Categories with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    client.create_realtime_endpoint(\n",
    "        MLModelId='ProdutoCategorias'\n",
    "    )\n",
    "    print('Please, wait a few minutes while the endpoint is being created. Get some coffee...')\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_category( product_name ):\n",
    "    response = client.predict(\n",
    "        MLModelId='ProdutoCategorias',\n",
    "        Record={\n",
    "            'product_name': product_name\n",
    "        },\n",
    "        PredictEndpoint='https://realtime.machinelearning.us-east-1.amazonaws.com'\n",
    "    )\n",
    "    return response['Prediction']['predictedLabel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testes = pd.read_csv(base_dir + '/testes.csv', sep=',', encoding='utf-8')\n",
    "testes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testes['predicted_category'] = testes['product_name'].apply(predict_category)\n",
    "testes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Well Done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
